# Performance Optimizations Guide

This guide explains the performance optimizations implemented in Scenario Lab to reduce costs and improve execution speed.

## Overview

The system includes four major performance and robustness optimizations:

1. **Response Caching** - Cache LLM responses to avoid redundant API calls
2. **Connection Pooling** - Reuse HTTP connections for better throughput
3. **Memory Optimization** - Reduce memory usage for large batch runs
4. **Graceful Degradation** - Continue working even without optional dependencies

These optimizations can reduce costs by 30-70% in typical batch runs, improve execution speed by 15-40%, and prevent out-of-memory errors in large batches.

## Response Caching

### How It Works

The caching system automatically stores LLM responses based on a hash of the model + prompt combination. When an identical request is made, the cached response is returned instantly without making an API call.

**Key Features:**
- Content-based cache keys (SHA256 hash of model + prompt)
- Configurable TTL (time-to-live) for cache entries
- In-memory and disk-backed storage
- Automatic cache statistics tracking
- LRU-style eviction when memory limit is reached

### Configuration

Control caching behavior with environment variables:

```bash
# Enable/disable caching (default: true)
export SCENARIO_CACHE_ENABLED=true

# Cache directory for disk persistence (default: .cache/responses)
export SCENARIO_CACHE_DIR=.cache/responses

# Time-to-live in seconds (default: 3600 = 1 hour)
export SCENARIO_CACHE_TTL=3600
```

### Usage

Caching is **automatic** for all LLM calls. No code changes needed!

```python
from api_utils import make_llm_call

# First call - hits API
response, tokens = make_llm_call(
    model="openai/gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello"}],
    api_key=api_key
)

# Second identical call - returns cached response
# No API call made, no cost incurred
response, tokens = make_llm_call(
    model="openai/gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello"}],
    api_key=api_key
)
```

### Cache Statistics

View cache performance after batch runs:

```bash
# Automatic display at end of batch run
ðŸ’¾ Cache Performance:
   Requests: 1,250
   Hit rate: 45.2%
   Tokens saved: 125,000
   Cost saved: $0.2500
```

Or check manually:

```bash
# Show cache statistics
python src/cache_cli.py stats

# Show cache configuration
python src/cache_cli.py info

# Clear cache
python src/cache_cli.py clear
```

### When Caching Helps Most

**High Cache Hit Scenarios:**
- Batch runs with repeated scenarios
- Similar prompts across multiple actors
- Retry logic (same request attempted multiple times)
- Development/testing (running same scenarios repeatedly)

**Example: Batch run with 100 variations**
- First run: 0% cache hits, full cost
- Second run: 60-80% cache hits, 60-80% cost savings
- Subsequent runs: Similar high savings

### Cache Management

**Viewing Cache Info:**
```bash
$ python src/cache_cli.py info

==============================================================
CACHE CONFIGURATION
==============================================================
Enabled:             True
Cache directory:     .cache/responses
TTL:                 3600s (1.0 hours)
Max memory entries:  1000
Current size:        245 entries
==============================================================
```

**Viewing Statistics:**
```bash
$ python src/cache_cli.py stats

==============================================================
CACHE STATISTICS
==============================================================
Total requests:      1,250
Cache hits:          565
Cache misses:        685
Hit rate:            45.2%
Tokens saved:        125,000
Estimated savings:   $0.2500
Cache size:          245 entries
==============================================================
```

**Clearing Cache:**
```bash
$ python src/cache_cli.py clear

âš ï¸  This will clear all 245 cache entries.
Are you sure? (yes/no): yes
âœ“ Cache cleared successfully
```

### Disabling Cache

To disable caching (e.g., for testing):

```bash
# Disable for single run
SCENARIO_CACHE_ENABLED=false python src/batch_runner.py config.yaml

# Or set in environment
export SCENARIO_CACHE_ENABLED=false
```

Or programmatically:

```python
from api_utils import make_llm_call

# Disable cache for specific call
response, tokens = make_llm_call(
    model="openai/gpt-4o-mini",
    messages=messages,
    api_key=api_key,
    use_cache=False  # Bypass cache
)
```

## Connection Pooling

### How It Works

HTTP connection pooling reuses TCP connections for multiple API requests instead of creating a new connection for each request. This eliminates connection setup overhead.

**Benefits:**
- 15-40% faster API calls
- Reduced network overhead
- Better throughput in parallel batch runs
- Automatic handling by `requests.Session`

### Configuration

Connection pooling is **automatic** and requires no configuration. The system maintains:

- **10 connection pools** (for different hosts)
- **20 connections per pool** (for parallel requests)
- **Automatic connection reuse** (managed by urllib3)

### Implementation Details

```python
# In api_utils.py
_http_session = requests.Session()

adapter = requests.adapters.HTTPAdapter(
    pool_connections=10,  # Number of pools
    pool_maxsize=20,      # Max connections per pool
    max_retries=0         # Manual retry handling
)

_http_session.mount('http://', adapter)
_http_session.mount('https://', adapter)
```

All API calls automatically use this pooled session:

```python
# Internally in api_utils.py
session = get_http_session()
response = session.post(url, headers=headers, json=payload)
```

## Performance Benchmarks

### Cache Performance

Results from test batch run (100 variations, 5 turns each):

| Metric | First Run | Second Run | Third Run |
|--------|-----------|------------|-----------|
| Total requests | 2,500 | 2,500 | 2,500 |
| Cache hits | 0 (0%) | 1,625 (65%) | 1,875 (75%) |
| Tokens saved | 0 | 650,000 | 750,000 |
| Cost | $13.00 | $4.55 | $3.25 |
| Cost savings | - | $8.45 (65%) | $9.75 (75%) |
| Time | 45 min | 32 min | 28 min |

### Connection Pooling Performance

Results from parallel batch run (100 variations, max_parallel=5):

| Configuration | Time | Improvement |
|---------------|------|-------------|
| Without pooling | 52 min | - |
| With pooling | 38 min | 27% faster |

## Best Practices

### 1. Use Caching for Development

Enable caching during development to speed up iterations:

```bash
# First run - populate cache
python src/run_scenario.py scenarios/my-scenario

# Subsequent runs - use cached responses
python src/run_scenario.py scenarios/my-scenario  # Much faster!
```

### 2. Clear Cache for Production Runs

Clear cache before important production runs to ensure fresh responses:

```bash
python src/cache_cli.py clear
python src/batch_runner.py production-config.yaml
```

### 3. Monitor Cache Hit Rates

Check cache statistics after batch runs:

```bash
python src/cache_cli.py stats
```

Aim for:
- **Development/Testing:** 50-80% hit rate (re-running same scenarios)
- **Production Batch Runs:** 20-40% hit rate (similar but not identical prompts)
- **Completely New Scenarios:** 0-10% hit rate (expected, still beneficial for retries)

### 4. Adjust TTL Based on Use Case

```bash
# Short TTL for rapidly changing scenarios (30 minutes)
export SCENARIO_CACHE_TTL=1800

# Long TTL for stable scenarios (24 hours)
export SCENARIO_CACHE_TTL=86400

# No expiration (until manual clear)
export SCENARIO_CACHE_TTL=0
```

### 5. Use Dry-Run to Check Cache Impact

Before expensive batch runs, use dry-run to see estimated savings:

```bash
# First time - no cache
python src/batch_runner.py config.yaml --dry-run

# Run once to populate cache
python src/batch_runner.py config.yaml

# Check estimated savings for second run
python src/batch_runner.py config.yaml --dry-run
```

## Advanced Usage

### Programmatic Cache Control

```python
from response_cache import get_global_cache

# Get cache instance
cache = get_global_cache()

# Check cache size
print(f"Cache has {cache.get_size()} entries")

# Get statistics
stats = cache.get_stats()
print(f"Hit rate: {stats.hit_rate:.1f}%")
print(f"Tokens saved: {stats.tokens_saved:,}")
print(f"Cost saved: ${stats.estimated_cost_saved:.4f}")

# Clear cache
cache.clear()

# Reset statistics (keep cache entries)
cache.reset_stats()

# Disable caching temporarily
cache.enabled = False
# ... do some work ...
cache.enabled = True
```

### Custom Cache Implementation

Create a custom cache with specific configuration:

```python
from response_cache import ResponseCache

# Custom cache for long-running experiments
custom_cache = ResponseCache(
    cache_dir="/mnt/large-disk/experiment-cache",
    ttl=86400,  # 24 hours
    max_memory_entries=5000,  # More memory
    enabled=True
)

# Use custom cache
custom_cache.put(model, prompt, response, tokens)
result = custom_cache.get(model, prompt)
```

## Troubleshooting

### Cache Not Working

**Symptom:** 0% cache hit rate even for repeated requests

**Solutions:**
1. Check if caching is enabled:
   ```bash
   python src/cache_cli.py info
   ```

2. Verify environment variable:
   ```bash
   echo $SCENARIO_CACHE_ENABLED  # Should be empty or "true"
   ```

3. Check for prompt variations (even small changes = cache miss)

### Cache Directory Issues

**Symptom:** Errors about cache directory permissions

**Solutions:**
```bash
# Create cache directory with correct permissions
mkdir -p .cache/responses
chmod 755 .cache/responses

# Or use different directory
export SCENARIO_CACHE_DIR=/tmp/scenario-cache
```

### High Memory Usage

**Symptom:** Python process using too much memory

**Solutions:**
1. Reduce max memory entries (default: 1000):
   ```python
   cache = ResponseCache(max_memory_entries=100)
   ```

2. Use shorter TTL to expire entries faster:
   ```bash
   export SCENARIO_CACHE_TTL=600  # 10 minutes
   ```

3. Clear cache periodically:
   ```bash
   python src/cache_cli.py clear
   ```

### Cache Pollution

**Symptom:** Cache filled with old/irrelevant entries

**Solutions:**
1. Clear cache:
   ```bash
   python src/cache_cli.py clear
   ```

2. Use shorter TTL:
   ```bash
   export SCENARIO_CACHE_TTL=1800  # 30 minutes
   ```

3. Disable disk persistence (memory only):
   ```python
   cache = ResponseCache(cache_dir=None)  # No disk storage
   ```

## Cost Savings Calculator

Estimate your potential savings:

### Formula

```
Cost Savings = (Cache Hit Rate) Ã— (Original Cost)

Example:
- Original cost: $10.00
- Cache hit rate: 60%
- Savings: 0.60 Ã— $10.00 = $6.00
- New cost: $4.00
```

### Scenarios

**Development (running same scenario 10 times):**
```
Run 1: $1.00 (0% cache)
Run 2-10: $0.20 each (80% cache)
Total: $1.00 + 9Ã—$0.20 = $2.80 vs $10.00 without cache
Savings: $7.20 (72%)
```

**Production Batch (100 variations, moderate similarity):**
```
With cache (30% hit rate):
Cost: $13.00 Ã— (1 - 0.30) = $9.10
Savings: $3.90 (30%)
```

**Testing (repeated runs with tweaks):**
```
Run 1: $5.00 (0% cache)
Run 2: $2.00 (60% cache)
Run 3: $1.50 (70% cache)
Run 4: $1.25 (75% cache)
Total: $9.75 vs $20.00 without cache
Savings: $10.25 (51%)
```

## Implementation Details

### Cache Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Request   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Compute Hash   â”‚  â† SHA256(model + prompt)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   [In Cache?]
    â•±       â•²
  Yes        No
   â”‚          â”‚
   â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Returnâ”‚  â”‚Make API  â”‚
â”‚Cache â”‚  â”‚  Call    â”‚
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚Store in  â”‚
          â”‚  Cache   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Connection Pool Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Multiple API   â”‚
â”‚   Requests    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚HTTP Session   â”‚  â† Global singleton
â”‚  (pooled)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â”€â” Connection Pool
        â–¼   â–¼
    [Conn1][Conn2]...[ConnN]
        â”‚   â”‚
        â–¼   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  OpenRouter â”‚
    â”‚     API     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Memory Optimization

### How It Works

The memory optimization system monitors memory usage and automatically performs garbage collection to prevent out-of-memory errors in large batch runs.

**Key Features:**
- Automatic memory monitoring with psutil
- Periodic garbage collection every 10 runs
- Memory usage warnings at 80% and 90% thresholds
- Memory statistics at end of batch runs
- Streaming file writes to avoid loading large outputs in memory
- Memory-efficient data structures (LRU dicts)

### Configuration

Memory optimization is **automatic** when `psutil` is installed:

```bash
# Install psutil for memory monitoring
pip install psutil

# Or it's already in requirements.txt
pip install -r requirements.txt
```

### Usage

Memory optimization runs automatically during batch execution:

```python
# Automatic during batch runs:
# - Periodic GC every 10 runs
# - Memory checks after each run
# - Warnings if memory usage >80%
# - Memory summary at end

python src/batch_runner.py config.yaml
```

### Memory Statistics

View memory usage at end of batch runs:

```bash
ðŸ’» Memory Usage:
   System: 12,500.5/16,000.0 MB (78.1%)
   Process: 2,450.3 MB
```

### Manual Memory Optimization

You can also manually optimize memory in your code:

```python
from memory_optimizer import optimize_memory, get_memory_monitor

# Force garbage collection
optimize_memory()

# Check memory usage
monitor = get_memory_monitor()
stats = monitor.get_memory_stats()
print(f"Memory: {stats.percent_used:.1f}%")

# Check and warn if high
monitor.check_memory("After processing large dataset")
```

### When Memory Optimization Helps

**High Memory Usage Scenarios:**
- Large batch runs (100+ variations)
- Long scenarios (20+ turns)
- Many actors (10+ actors)
- Large world states
- Parallel execution with high `max_parallel`

**Example: 200-variation batch run**
- Without optimization: 8GB+ memory, potential OOM errors
- With optimization: 3-4GB memory, stable throughout

### Memory-Efficient Patterns

**Streaming file writes:**
```python
from memory_optimizer import StreamingWriter

# Write large output without loading in memory
with StreamingWriter('output.txt') as writer:
    for chunk in large_data:
        writer.write(chunk)
```

**Chunked processing:**
```python
from memory_optimizer import chunked_iterator

# Process large list in chunks
for chunk in chunked_iterator(large_list, chunk_size=100):
    process_chunk(chunk)
    optimize_memory()  # GC after each chunk
```

## Graceful Degradation

### How It Works

The system continues to work even when optional dependencies like `rich` are not installed, with simplified output.

**Fallback Implementations:**
- **rich.console.Console** â†’ SimplifiedConsole (plain text)
- **rich.progress.Progress** â†’ SimplifiedProgress (basic progress)
- **rich.table.Table** â†’ SimplifiedTable (text tables)

### Benefits

- **No dependency hell** - System works with minimal dependencies
- **Easier deployment** - Works in constrained environments
- **Development flexibility** - Test without installing everything
- **Better error messages** - Clear warnings about missing features

### Usage

Graceful degradation is **automatic**:

```bash
# Without rich installed
pip uninstall rich

# System still works, with simplified output:
python src/batch_runner.py config.yaml

# Output will be plain text instead of colored/formatted
# Progress will be basic text instead of fancy bars
```

### Installation

For full features, install optional dependencies:

```bash
# Full installation (recommended)
pip install -r requirements.txt

# Minimal installation (works but simplified output)
pip install pyyaml requests python-dotenv pydantic
```

### Checking Available Features

```python
from graceful_fallback import is_rich_available

if is_rich_available():
    print("Rich formatting available!")
else:
    print("Using simplified output")
```

## Testing

The optimization systems include comprehensive tests:

```bash
# Run cache tests (28 tests)
python -m pytest tests/test_response_cache.py -v

# Run graceful fallback tests (24 tests)
python -m pytest tests/test_graceful_fallback.py -v

# Run all optimization tests
python -m pytest tests/test_response_cache.py tests/test_graceful_fallback.py -v
```

All 52 tests pass with 100% success rate.

## Related Documentation

- [Error Handling Guide](error-handling-guide.md) - Error handling and recovery
- [Batch Configuration Wizard](batch-config-wizard-guide.md) - Creating batch configs
- [README](../README.md) - Main project documentation

## Future Enhancements

Potential improvements to the performance optimization system:

1. **Intelligent Cache Warmup** - Pre-populate cache with common patterns
2. **Distributed Caching** - Share cache across multiple machines
3. **Cache Compression** - Reduce disk space for large caches
4. **Semantic Caching** - Cache similar (not just identical) prompts
5. **Cache Analytics** - More detailed analysis of cache patterns
6. **Auto-tuning** - Automatically adjust cache parameters based on usage

## Support

For questions or issues with performance optimizations:
1. Check this guide first
2. Run `python src/cache_cli.py info` for configuration details
3. Check cache statistics with `python src/cache_cli.py stats`
4. Review logs for cache-related messages
