# AI 2027: Race to Superintelligence

A detailed scenario exploring AI development from mid-2025 through 2030, examining the decisions made by AI companies, governments, and alignment researchers during a period of rapid capability advancement.

## Scenario Overview

This scenario is based on "AI 2027" by Daniel Kokotajlo, Scott Alexander, Thomas Larsen, Eli Lifland, and Romeo Dean. It models the progression from early "stumbling agents" (Agent-0) to potentially superintelligent systems, focusing on:

- Competitive pressures between US and Chinese AI development
- Alignment research racing against capability advancement
- Government policy responses to transformative AI
- Economic and social impacts of increasing automation
- Critical decision points that determine whether humanity maintains control

## Running the Scenario

### Basic Run

From the repository root:

```bash
python src/run_scenario.py scenarios/ai-2027/definition/scenario.yaml
```

This will:
- Load the scenario configuration and all 7 actors
- Run for 66 turns (months) from mid-2025 to end of 2030
- Generate markdown documentation for each turn
- Track metrics on AI capabilities, alignment progress, cooperation, etc.
- Perform validation checks on consistency and realism
- Create cost reports for LLM API usage

### Resumable Runs

The scenario is long (66 turns) and may hit rate limits or budget constraints. You can:

**Set limits:**
```bash
python src/run_scenario.py scenarios/ai-2027/definition/scenario.yaml --max-turns 12 --credit-limit 50
```

**Resume from a halted run:**
```bash
python src/run_scenario.py --resume scenarios/ai-2027/runs/run-001
```

### Scenario Branching

To explore "what-if" alternatives from any point:

```bash
python src/run_scenario.py --branch-from scenarios/ai-2027/runs/run-001 --branch-at-turn 24
```

This creates a new run starting from turn 24 of run-001, allowing you to:
- Modify actor prompts or goals
- Test different policy responses
- Explore alternative technical developments
- Compare outcomes from the same starting point

## Directory Structure

```
ai-2027/
├── README.md                          # This file
├── ai-2027.pdf                        # Original scenario document
├── definition/
│   ├── scenario.yaml                  # Scenario configuration
│   ├── metrics.yaml                   # Tracked metrics definitions
│   ├── validation-rules.yaml          # QA validation configuration
│   ├── actors/
│   │   ├── openbrain-ceo.yaml        # OpenBrain CEO
│   │   ├── openbrain-alignment-lead.yaml
│   │   ├── us-president.yaml
│   │   ├── us-ai-advisor.yaml
│   │   ├── deepcent-ceo.yaml         # Chinese AI company CEO
│   │   ├── ccp-secretary.yaml        # Chinese government
│   │   └── independent-alignment-researcher.yaml
│   └── background/
│       ├── scenario-overview.md       # Scenario context and goals
│       └── technical-concepts.md      # Key concepts explained
├── runs/                              # Generated during execution
│   └── run-NNN/                       # Each run in separate directory
│       ├── world-state-001.md         # World state each turn
│       ├── actor-name-001.md          # Actor decisions each turn
│       ├── metrics.json               # Tracked metrics over time
│       ├── costs.json                 # LLM API costs
│       ├── validation-NNN.md          # Validation reports per turn
│       ├── validation-summary.md      # Overall validation summary
│       └── scenario-state.json        # State for resumption
└── analysis/                          # Generated by analysis tools
    ├── statistics.md                  # Statistical analysis across runs
    └── metrics-summary.json           # Aggregated metrics
```

## Key Actors

### Private Sector
- **OpenBrain CEO**: Leading AI company, balancing innovation and safety
- **OpenBrain Alignment Lead**: Head of safety research, advocating for caution
- **DeepCent CEO**: Chinese competitor trying to catch up

### Government
- **US President**: High-level policy decisions on AI governance
- **US AI Advisor**: Technical expertise and policy recommendations
- **CCP Secretary**: Chinese leadership overseeing national AI strategy

### Independent
- **Independent Alignment Researcher**: External safety advocate

## Tracked Metrics

The scenario tracks 10 key metrics:

1. **AI Capability Level** (0-10): From Agent-0 to superintelligence
2. **Alignment Progress** (0-10): Safety research advancement
3. **US-China Cooperation** (0-10): Competition vs. coordination
4. **Regulatory Stringency** (0-10): Strength of governance
5. **Public Risk Awareness** (0-10): Public concern about AI risks
6. **Misalignment Risk Level** (0-10): Assessed catastrophe risk
7. **Compute Scale** (25-32): Training compute in log scale (10^N FLOP)
8. **Economic Automation** (0-10): Degree of AI automation
9. **Human Control Level** (0-10): Meaningful human oversight
10. **Outcome Trajectory**: Race ending, slowdown ending, or uncertain

## Validation

The scenario includes comprehensive validation checking:

- **Actor Decision Consistency**: Decisions align with goals and expertise
- **World State Coherence**: Updates follow logically from actions
- **Information Access**: Actors only use information they have
- **Technical Realism**: AI progression follows plausible timelines
- **Geopolitical Realism**: US-China dynamics are realistic

Validation runs automatically and generates per-turn reports plus a summary.

## Key Decision Points

Critical junctures likely to emerge:

1. **Scaling decisions**: When to move from Agent-N to Agent-N+1
2. **Safety measures**: Whether to implement costly alignment research
3. **Regulatory responses**: How governments react to capability jumps
4. **International coordination**: Attempts at treaties or agreements
5. **Incident responses**: How actors react to alignment failures or near-misses
6. **Economic disruption**: Managing automation's societal impact
7. **Control thresholds**: When AI systems become too complex to oversee

## Expected Outcomes

The scenario can branch toward different endings:

**Race Ending**: Competitive pressures override safety, leading to misaligned AI takeover

**Slowdown Ending**: International coordination enables safe development

**Other Outcomes**: Intermediate scenarios, partial automation, different failure modes

The actual trajectory depends on actor decisions throughout the simulation.

## Interpreting Results

When analyzing runs, consider:

- **Metrics trajectories**: How did key indicators evolve?
- **Decision quality**: Were choices well-reasoned given available information?
- **Critical moments**: Which decisions proved most consequential?
- **Counterfactuals**: How might different choices have changed outcomes?
- **Actor dynamics**: How did competition and cooperation evolve?
- **Validation findings**: Were there consistency issues or unrealistic developments?

## Batch Analysis

For statistical analysis across multiple runs:

```bash
python src/batch_runner.py scenarios/ai-2027/definition/scenario.yaml --runs 100
```

This enables:
- Identifying most likely outcome trajectories
- Finding critical factors that determine outcomes
- Testing robustness of different policies
- Sensitivity analysis on actor behaviors

## Credits

This scenario is based on "AI 2027" by:
- Daniel Kokotajlo
- Scott Alexander
- Thomas Larsen
- Eli Lifland
- Romeo Dean

Full document: https://ai-2027.com/ai-2027.pdf

## License

The original AI 2027 document is used with permission for research purposes. This scenario configuration is provided for AI governance research and policy analysis.
