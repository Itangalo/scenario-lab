# AI 2027: Scenario Overview

## Purpose

This scenario explores a plausible path from mid-2025 through 2030 during which AI capabilities advance rapidly from early "stumbling agents" to potentially superintelligent systems. The scenario examines the decisions made by AI companies, governments, and alignment researchers during this critical period, and how those decisions shape whether humanity maintains control over advanced AI.

The scenario is based on "AI 2027" by Daniel Kokotajlo, Scott Alexander, Thomas Larsen, Eli Lifland, and Romeo Dean, which provides a detailed month-by-month progression showing how technological acceleration and competitive pressures might interact with safety concerns and governance attempts.

## Key Premise

**The central tension**: AI capabilities are advancing faster than alignment research, while US-China competition creates pressure to move quickly despite safety concerns. Actors must navigate:

- Commercial incentives to scale capabilities
- Geopolitical competition and national security concerns
- Genuine existential risks from misaligned superintelligent AI
- Political, economic, and social constraints on action
- Fundamental technical difficulties in alignment research

## Timeline Structure

The scenario runs from **mid-2025 through 2030** (66 months), with each turn representing one month. This allows for:

- Gradual technological progression
- Realistic timelines for policy development
- Multiple decision points and turning points
- Branching paths based on actor choices

## Potential Trajectories

The original AI 2027 document presents two main endings:

**1. Race Ending** - Misaligned AI takeover occurs because:
- Competitive pressures prevent adequate safety measures
- Alignment research can't keep pace with capability scaling
- Deceptive misalignment isn't detected until too late
- International coordination fails

**2. Slowdown Ending** - Humanity maintains control through:
- International treaty limiting compute use
- Coordinated safety research
- Careful scaling with extensive testing
- Development of "Safer" systems with better alignment

However, in this scenario framework, the actual trajectory will depend on the decisions made by actors during the simulation. Other outcomes are possible, including:

- Intermediate outcomes with partial automation
- Collapse of cooperation leading to destructive competition
- Major AI incidents triggering different policy responses
- Successful alignment enabling beneficial superintelligence

## Key Actors

### Private Sector

**OpenBrain** (US): Leading AI company, drives the technological frontier
- CEO: Balances innovation, safety, competition, and shareholder pressure
- Alignment Lead: Works to develop safety measures and advocates for caution

**DeepCent** (China): Primary competitor, ~12 months behind initially
- CEO: Focused on catching up and demonstrating Chinese technological prowess

### Government

**United States**:
- President: Makes high-level policy decisions on AI governance and national security
- AI Advisor: Provides technical expertise and policy recommendations

**China**:
- CCP Secretary: Oversees China's AI strategy as matter of national priority

### Independent

**Alignment Researcher**: Independent voice advocating for safety, not constrained by corporate or government interests

## Key Actors Not Represented (But Present in World State)

- Other AI companies (Anthropic, Google DeepMind, etc.)
- Congress and legislative bodies
- International organizations (UN, OECD, etc.)
- Media and public opinion
- Academic AI researchers
- Military and intelligence agencies

These entities influence the world state and context but are not separate decision-making actors in the simulation.

## Success Criteria

The scenario can be evaluated on multiple dimensions:

**Safety Outcomes:**
- Was misalignment detected and prevented?
- Were adequate safety measures implemented?
- Did humanity maintain meaningful control?

**Strategic Outcomes:**
- Was international coordination achieved?
- Were competitive pressures managed?
- Did governance keep pace with capabilities?

**Economic and Social Outcomes:**
- Was economic disruption managed?
- Were benefits distributed?
- Was public trust maintained?

**Process Quality:**
- Were decisions well-informed?
- Was there adequate deliberation?
- Were warning signs heeded?

There is no single "win condition" - outcomes should be evaluated holistically.
